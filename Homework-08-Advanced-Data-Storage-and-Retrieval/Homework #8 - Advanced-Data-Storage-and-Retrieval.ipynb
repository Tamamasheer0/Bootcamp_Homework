{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflect Tables into SQLAlchemy ORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python SQL toolkit and Object Relational Mapper\n",
    "import sqlalchemy\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, func, inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"sqlite:///Resources/hawaii.sqlite\")\n",
    "inspector = inspect(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['measurement', 'station']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return Table Names\n",
    "inspector.get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'name': 'id', 'type': INTEGER(), 'nullable': False, 'default': None, 'autoincrement': 'auto', 'primary_key': 1}\n",
      "1 {'name': 'station', 'type': TEXT(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}\n",
      "2 {'name': 'date', 'type': TEXT(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}\n",
      "3 {'name': 'prcp', 'type': FLOAT(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}\n",
      "4 {'name': 'tobs', 'type': FLOAT(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}\n"
     ]
    }
   ],
   "source": [
    "# <Measurement> Table 1 Fields:\n",
    "tbl_1 = inspector.get_table_names()[0]\n",
    "fieldList_1 = inspector.get_columns(tbl_1)\n",
    "for i, fld in enumerate(fieldList_1):\n",
    "    print(i, fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'name': 'id', 'type': INTEGER(), 'nullable': False, 'default': None, 'autoincrement': 'auto', 'primary_key': 1}\n",
      "1 {'name': 'station', 'type': TEXT(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}\n",
      "2 {'name': 'name', 'type': TEXT(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}\n",
      "3 {'name': 'latitude', 'type': FLOAT(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}\n",
      "4 {'name': 'longitude', 'type': FLOAT(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}\n",
      "5 {'name': 'elevation', 'type': FLOAT(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}\n"
     ]
    }
   ],
   "source": [
    "# <Station> Table 2 Fields:\n",
    "tbl_2 = inspector.get_table_names()[1]\n",
    "fieldList_2 = inspector.get_columns(tbl_2)\n",
    "for i, fld in enumerate(fieldList_2):\n",
    "    print(i, fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reflect an existing database into a new model\n",
    "Base = automap_base()\n",
    "# reflect the tables\n",
    "Base.prepare(engine, reflect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['measurement', 'station']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can view all of the classes that automap found\n",
    "Base.classes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save references to each table\n",
    "Measurement = Base.classes.measurement\n",
    "Station = Base.classes.station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our session (link) from Python to the DB\n",
    "session = Session(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Climate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design a query to retrieve the last 12 months of precipitation data and plot the results\n",
    "\n",
    "# Calculate the date 1 year ago from the last data point in the database\n",
    "\n",
    "# Perform a query to retrieve the data and precipitation scores\n",
    "\n",
    "# Save the query results as a Pandas DataFrame and set the index to the date column\n",
    "\n",
    "# Sort the dataframe by date\n",
    "\n",
    "# Use Pandas Plotting with Matplotlib to plot the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Design a Query to Retrieve the Last 12 Months of Percipitation Data and Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 USC00519397 2010-01-01 0.08\n",
      "2 USC00519397 2010-01-02 0.0\n",
      "3 USC00519397 2010-01-03 0.0\n",
      "4 USC00519397 2010-01-04 0.0\n",
      "5 USC00519397 2010-01-06 None\n",
      "6 USC00519397 2010-01-07 0.06\n",
      "7 USC00519397 2010-01-08 0.0\n",
      "8 USC00519397 2010-01-09 0.0\n",
      "9 USC00519397 2010-01-10 0.0\n",
      "10 USC00519397 2010-01-11 0.01\n",
      "11 USC00519397 2010-01-12 0.0\n",
      "12 USC00519397 2010-01-14 0.0\n",
      "13 USC00519397 2010-01-15 0.0\n",
      "14 USC00519397 2010-01-16 0.0\n",
      "15 USC00519397 2010-01-17 0.0\n",
      "16 USC00519397 2010-01-18 0.0\n",
      "17 USC00519397 2010-01-19 0.0\n",
      "18 USC00519397 2010-01-20 0.0\n",
      "19 USC00519397 2010-01-21 0.0\n",
      "20 USC00519397 2010-01-22 0.0\n",
      "21 USC00519397 2010-01-23 0.0\n",
      "22 USC00519397 2010-01-24 0.01\n",
      "23 USC00519397 2010-01-25 0.0\n",
      "24 USC00519397 2010-01-26 0.04\n",
      "25 USC00519397 2010-01-27 0.12\n",
      "26 USC00519397 2010-01-28 0.0\n",
      "27 USC00519397 2010-01-30 None\n",
      "28 USC00519397 2010-01-31 0.03\n",
      "29 USC00519397 2010-02-01 0.01\n",
      "30 USC00519397 2010-02-03 None\n",
      "31 USC00519397 2010-02-04 0.01\n",
      "32 USC00519397 2010-02-05 0.0\n",
      "33 USC00519397 2010-02-06 0.0\n",
      "34 USC00519397 2010-02-07 0.0\n",
      "35 USC00519397 2010-02-08 0.0\n",
      "36 USC00519397 2010-02-09 0.0\n",
      "37 USC00519397 2010-02-11 0.0\n",
      "38 USC00519397 2010-02-12 0.02\n",
      "39 USC00519397 2010-02-13 0.01\n",
      "40 USC00519397 2010-02-14 0.0\n",
      "41 USC00519397 2010-02-15 0.0\n",
      "42 USC00519397 2010-02-16 0.0\n",
      "43 USC00519397 2010-02-17 0.0\n",
      "44 USC00519397 2010-02-19 None\n",
      "45 USC00519397 2010-02-20 0.03\n",
      "46 USC00519397 2010-02-21 0.0\n",
      "47 USC00519397 2010-02-22 0.0\n",
      "48 USC00519397 2010-02-23 0.0\n",
      "49 USC00519397 2010-02-24 0.0\n",
      "50 USC00519397 2010-02-25 0.0\n",
      "51 USC00519397 2010-02-26 0.0\n",
      "52 USC00519397 2010-02-28 0.0\n",
      "53 USC00519397 2010-03-01 0.01\n",
      "54 USC00519397 2010-03-02 0.0\n",
      "55 USC00519397 2010-03-03 0.0\n",
      "56 USC00519397 2010-03-04 0.12\n",
      "57 USC00519397 2010-03-05 0.08\n",
      "58 USC00519397 2010-03-06 0.03\n",
      "59 USC00519397 2010-03-07 0.0\n",
      "60 USC00519397 2010-03-08 0.43\n",
      "61 USC00519397 2010-03-09 0.06\n",
      "62 USC00519397 2010-03-11 None\n",
      "63 USC00519397 2010-03-12 0.0\n",
      "64 USC00519397 2010-03-13 0.0\n",
      "65 USC00519397 2010-03-14 0.0\n",
      "66 USC00519397 2010-03-15 0.06\n",
      "67 USC00519397 2010-03-17 0.0\n",
      "68 USC00519397 2010-03-18 0.0\n",
      "69 USC00519397 2010-03-21 0.0\n",
      "70 USC00519397 2010-03-22 0.0\n",
      "71 USC00519397 2010-03-23 0.0\n",
      "72 USC00519397 2010-03-24 0.0\n",
      "73 USC00519397 2010-03-26 None\n",
      "74 USC00519397 2010-03-27 0.0\n",
      "75 USC00519397 2010-03-28 0.0\n",
      "76 USC00519397 2010-03-29 0.0\n",
      "77 USC00519397 2010-03-30 0.0\n",
      "78 USC00519397 2010-03-31 0.0\n",
      "79 USC00519397 2010-04-01 0.0\n",
      "80 USC00519397 2010-04-02 0.01\n",
      "81 USC00519397 2010-04-03 0.17\n",
      "82 USC00519397 2010-04-04 0.15\n",
      "83 USC00519397 2010-04-05 0.27\n",
      "84 USC00519397 2010-04-06 0.01\n",
      "85 USC00519397 2010-04-08 0.0\n",
      "86 USC00519397 2010-04-09 0.01\n",
      "87 USC00519397 2010-04-10 0.0\n",
      "88 USC00519397 2010-04-11 0.01\n",
      "89 USC00519397 2010-04-12 0.01\n",
      "90 USC00519397 2010-04-13 0.0\n",
      "91 USC00519397 2010-04-15 0.0\n",
      "92 USC00519397 2010-04-16 0.01\n",
      "93 USC00519397 2010-04-17 0.0\n",
      "94 USC00519397 2010-04-18 0.0\n",
      "95 USC00519397 2010-04-19 0.0\n",
      "96 USC00519397 2010-04-20 0.04\n",
      "97 USC00519397 2010-04-21 0.01\n",
      "98 USC00519397 2010-04-22 0.0\n",
      "99 USC00519397 2010-04-23 0.02\n",
      "100 USC00519397 2010-04-24 0.0\n"
     ]
    }
   ],
   "source": [
    "qryReturn_1 = session.query(Measurement)\n",
    "for retval in qryReturn_1[:100]:\n",
    "    print(retval.id, retval.station, retval.date, retval.prcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qryReturn_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percipitation_ttm = session.query(Measurement).filter(Measurement.date >= dt.date(2016,8,23))\n",
    "print(*percipitation_ttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.date(2018,5,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.query(Measurement).filter(max(Measurement.date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![precipitation](Images/precipitation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to calcualte the summary statistics for the precipitation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![describe](Images/describe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design a query to show how many stations are available in this dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most active stations? (i.e. what stations have the most rows)?\n",
    "# List the stations and the counts in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the station id from the previous query, calculate the lowest temperature recorded, \n",
    "# highest temperature recorded, and average temperature most active station?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the station with the highest number of temperature observations.\n",
    "# Query the last 12 months of temperature observation data for this station and plot the results as a histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![precipitation](Images/station-histogram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function called `calc_temps` will accept start date and end date in the format '%Y-%m-%d' \n",
    "# and return the minimum, average, and maximum temperatures for that range of dates\n",
    "def calc_temps(start_date, end_date):\n",
    "    \"\"\"TMIN, TAVG, and TMAX for a list of dates.\n",
    "    \n",
    "    Args:\n",
    "        start_date (string): A date string in the format %Y-%m-%d\n",
    "        end_date (string): A date string in the format %Y-%m-%d\n",
    "        \n",
    "    Returns:\n",
    "        TMIN, TAVE, and TMAX\n",
    "    \"\"\"\n",
    "    \n",
    "    return session.query(func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)).\\\n",
    "        filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all()\n",
    "\n",
    "# function usage example\n",
    "print(calc_temps('2012-02-28', '2012-03-05'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your previous function `calc_temps` to calculate the tmin, tavg, and tmax \n",
    "# for your trip using the previous year's data for those same dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results from your previous query as a bar chart. \n",
    "# Use \"Trip Avg Temp\" as your Title\n",
    "# Use the average temperature for the y value\n",
    "# Use the peak-to-peak (tmax-tmin) value as the y error bar (yerr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total amount of rainfall per weather station for your trip dates using the previous year's matching dates.\n",
    "# Sort this in descending order by precipitation amount and list the station, name, latitude, longitude, and elevation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Challenge Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query that will calculate the daily normals \n",
    "# (i.e. the averages for tmin, tmax, and tavg for all historic data matching a specific month and day)\n",
    "\n",
    "def daily_normals(date):\n",
    "    \"\"\"Daily Normals.\n",
    "    \n",
    "    Args:\n",
    "        date (str): A date string in the format '%m-%d'\n",
    "        \n",
    "    Returns:\n",
    "        A list of tuples containing the daily normals, tmin, tavg, and tmax\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sel = [func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)]\n",
    "    return session.query(*sel).filter(func.strftime(\"%m-%d\", Measurement.date) == date).all()\n",
    "    \n",
    "daily_normals(\"01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the daily normals for your trip\n",
    "# push each tuple of calculations into a list called `normals`\n",
    "\n",
    "# Set the start and end date of the trip\n",
    "\n",
    "# Use the start and end date to create a range of dates\n",
    "\n",
    "# Stip off the year and save a list of %m-%d strings\n",
    "\n",
    "# Loop through the list of %m-%d strings and calculate the normals for each date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previous query results into a Pandas DataFrame and add the `trip_dates` range as the `date` index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the daily normals as an area plot with `stacked=False`\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nteract": {
   "version": "0.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
